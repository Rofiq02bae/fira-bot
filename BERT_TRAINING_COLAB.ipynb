{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaedf375",
   "metadata": {},
   "source": [
    "# üöÄ BERT Training dengan Google Colab\n",
    "\n",
    "**Panduan Lengkap Training BERT Model untuk Chatbot**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Langkah-langkah:\n",
    "\n",
    "1. **Klik 'Runtime' > 'Change runtime type' > Ubah ke GPU T4**\n",
    "2. **Running cell pertama** - Install dependencies (2-3 menit)\n",
    "3. **Restart Session** setelah install selesai\n",
    "4. **Upload dataset** atau clone dari GitHub\n",
    "5. **Pilih model** yang akan digunakan (3 pilihan)\n",
    "6. **Running training** - Tunggu ~7-60 menit (tergantung dataset)\n",
    "7. **Download model** - Ekstrak ke `data/bert_model/`\n",
    "8. **Jalankan server** di local\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Estimasi Waktu:\n",
    "- Install Dependencies: **2-3 menit**\n",
    "- Upload Dataset: **1-2 menit**\n",
    "- Training BERT: **7-60 menit** (tergantung ukuran dataset)\n",
    "- Download Model: **2-3 menit**\n",
    "- **Total: ~15-70 menit**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è PENTING:\n",
    "‚úÖ Pastikan GPU sudah aktif (T4)  \n",
    "‚úÖ Restart session setelah install dependencies  \n",
    "‚úÖ Siapkan file `dataset_training.csv`  \n",
    "‚úÖ Simpan model setelah training selesai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb207bd",
   "metadata": {},
   "source": [
    "# üì¶ STEP 1: Install Dependencies\n",
    "\n",
    "**Copy-paste code di bawah dan jalankan**\n",
    "\n",
    "‚è±Ô∏è Waktu: ~2-3 menit\n",
    "\n",
    "‚ö†Ô∏è **PENTING:** Setelah selesai, **RESTART SESSION** untuk menerapkan perubahan!\n",
    "- Klik: **Runtime > Restart session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd2e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== INSTALL DEPENDENCIES ====================\n",
    "# Copy-paste ini di cell pertama Colab\n",
    "\n",
    "print(\"üîÑ Installing dependencies...\")\n",
    "print(\"‚è±Ô∏è  Estimasi waktu: 2-3 menit\\n\")\n",
    "\n",
    "# Install dengan versi yang sudah tested di Colab\n",
    "!pip install -q transformers datasets accelerate torch pandas\n",
    "!pip install -q scikit-learn numpy tqdm\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ Colab environment ready for BERT training!\")\n",
    "print(\"‚úÖ All dependencies installed successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ö†Ô∏è  PENTING: Restart Session sekarang!\")\n",
    "print(\"    Klik: Runtime > Restart session\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5284b",
   "metadata": {},
   "source": [
    "# üîç STEP 2: Verify GPU & Import Libraries\n",
    "\n",
    "Setelah **restart session**, jalankan cell ini untuk:\n",
    "- ‚úÖ Verifikasi GPU tersedia\n",
    "- ‚úÖ Import libraries\n",
    "- ‚úÖ Check CUDA availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VERIFY GPU ====================\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Check GPU\n",
    "print(\"üîç Checking GPU availability...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  GPU not available! Training will be slow.\")\n",
    "    print(\"   Pastikan Runtime Type = GPU T4\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"‚úÖ Device: {device}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify GPU with nvidia-smi\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898bd8c5",
   "metadata": {},
   "source": [
    "# üìÇ STEP 3: Upload Dataset\n",
    "\n",
    "**Pilih salah satu metode:**\n",
    "\n",
    "### Method 1: Upload Manual (Recommended)\n",
    "Jalankan cell di bawah, lalu upload file `dataset_training.csv`\n",
    "\n",
    "### Method 2: Clone dari GitHub\n",
    "Jika dataset sudah ada di repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c764241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== UPLOAD DATASET ====================\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üìÇ Upload file dataset_training.csv\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Upload dataset\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Verify upload\n",
    "if 'dataset_training.csv' in uploaded:\n",
    "    print(\"\\n‚úÖ Dataset uploaded successfully!\")\n",
    "    \n",
    "    # Preview dataset\n",
    "    df = pd.read_csv('dataset_training.csv')\n",
    "    print(f\"\\nüìä Dataset Info:\")\n",
    "    print(f\"   - Total rows: {len(df)}\")\n",
    "    print(f\"   - Columns: {list(df.columns)}\")\n",
    "    print(f\"\\nüîç Sample data:\")\n",
    "    print(df.head(3))\n",
    "    print(f\"\\nüìà Intent distribution:\")\n",
    "    print(df['tag'].value_counts().head(10))\n",
    "else:\n",
    "    print(\"\\n‚ùå Dataset not found! Please upload dataset_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2340e6cb",
   "metadata": {},
   "source": [
    "# üéØ STEP 4: Pilih Model BERT\n",
    "\n",
    "**Ada 3 pilihan model yang bisa digunakan:**\n",
    "\n",
    "| Model | Ukuran | Kecepatan | Akurasi | Recommended |\n",
    "|-------|--------|-----------|---------|-------------|\n",
    "| **IndoBERT Base** | ~500MB | Sedang | Tinggi | ‚úÖ **Production** |\n",
    "| **IndoBERT Lite** | ~200MB | Cepat | Sedang | Testing |\n",
    "| **mBERT** | ~700MB | Lambat | Tinggi | Multi-bahasa |\n",
    "\n",
    "**Pilih model dengan mengubah variabel `MODEL_CHOICE`:**\n",
    "- `1` = IndoBERT Base (Recommended)\n",
    "- `2` = IndoBERT Lite  \n",
    "- `3` = Multilingual BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baecc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== PILIH MODEL ====================\n",
    "\n",
    "# üéØ UBAH ANGKA DI BAWAH UNTUK PILIH MODEL (1, 2, atau 3)\n",
    "MODEL_CHOICE = 1  # Default: IndoBERT Base\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    1: {\n",
    "        'name': 'indobenchmark/indobert-base-p1',\n",
    "        'display_name': 'IndoBERT Base',\n",
    "        'description': 'üèÜ Best for production - High accuracy',\n",
    "        'size': '~500MB'\n",
    "    },\n",
    "    2: {\n",
    "        'name': 'indobenchmark/indobert-lite-base-p1',\n",
    "        'display_name': 'IndoBERT Lite',\n",
    "        'description': '‚ö° Fast training - Good for testing',\n",
    "        'size': '~200MB'\n",
    "    },\n",
    "    3: {\n",
    "        'name': 'cahya/bert-base-indonesian-522M',\n",
    "        'display_name': 'bert-base-indonesian-522M',\n",
    "        'description': 'indonesian-language support',\n",
    "        'size': '~700MB'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get selected model\n",
    "selected_model = MODELS[MODEL_CHOICE]\n",
    "MODEL_NAME = selected_model['name']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"üìå Model dipilih: {selected_model['display_name']}\")\n",
    "print(f\"   {selected_model['description']}\")\n",
    "print(f\"   Size: {selected_model['size']}\")\n",
    "print(f\"   Estimasi training: {selected_model['training_time']}\")\n",
    "print(f\"   Hugging Face: {MODEL_NAME}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1e8ab",
   "metadata": {},
   "source": [
    "# üèãÔ∏è STEP 5: Training BERT Model\n",
    "\n",
    "**Jalankan cell di bawah untuk mulai training**\n",
    "\n",
    "üìä **Proses:**\n",
    "1. Load & preprocess data\n",
    "2. Tokenize dataset\n",
    "3. Train model (3 epochs)\n",
    "4. Save model\n",
    "\n",
    "üí° **Tips:**\n",
    "- Jangan tutup tab Colab selama training\n",
    "- Monitor GPU usage\n",
    "- Training bisa dihentikan kapan saja (Ctrl+M I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430bf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== BERT TRAINING ====================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üèãÔ∏è  Starting BERT Training...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Configuration\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 128\n",
    "OUTPUT_DIR = './bert_model'\n",
    "\n",
    "print(f\"üìã Training Configuration:\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"   Max Length: {MAX_LENGTH}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load dataset\n",
    "print(\"\\nüìÇ Loading dataset...\")\n",
    "df = pd.read_csv('dataset_training.csv')\n",
    "\n",
    "# Prepare data\n",
    "texts = df['patterns'].astype(str).tolist()\n",
    "labels = df['tag'].astype(str).tolist()\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded:\")\n",
    "print(f\"   Total samples: {len(texts)}\")\n",
    "print(f\"   Unique intents: {num_labels}\")\n",
    "\n",
    "# Split data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
    ")\n",
    "\n",
    "print(f\"   Training samples: {len(train_texts)}\")\n",
    "print(f\"   Validation samples: {len(val_texts)}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(f\"\\nüîÑ Loading model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Tokenize data\n",
    "print(\"\\nüîÑ Tokenizing dataset...\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "\n",
    "# Create PyTorch dataset\n",
    "class IntentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IntentDataset(train_encodings, train_labels)\n",
    "val_dataset = IntentDataset(val_encodings, val_labels)\n",
    "\n",
    "print(\"‚úÖ Tokenization complete!\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ TRAINING STARTED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  Estimasi waktu: {selected_model['training_time']}\")\n",
    "print(\"üí° Anda bisa monitor GPU dengan: !nvidia-smi\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚è±Ô∏è  Total time: {duration:.2f} minutes\")\n",
    "print(f\"üìÅ Model saved to: {OUTPUT_DIR}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save model and tokenizer\n",
    "print(\"\\nüíæ Saving final model...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Save label encoder\n",
    "import pickle\n",
    "with open(f'{OUTPUT_DIR}/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "print(\"‚úÖ Model, tokenizer, and label encoder saved!\")\n",
    "print(\"\\nüéâ Training completed successfully!\")\n",
    "print(f\"üìÇ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4499e1a",
   "metadata": {},
   "source": [
    "# üß™ STEP 6: Test Model (Optional)\n",
    "\n",
    "Test model dengan beberapa contoh pertanyaan untuk verifikasi akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3c53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TEST MODEL ====================\n",
    "print(\"üß™ Testing trained model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test samples\n",
    "test_queries = [\n",
    "    \"jam buka bappenda\",\n",
    "    \"cara buat ktp\",\n",
    "    \"syarat nikah\",\n",
    "    \"bayar pajak online\"\n",
    "]\n",
    "\n",
    "def predict(text):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    \n",
    "    # Move to device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model.to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "    \n",
    "    # Decode label\n",
    "    intent = label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    return intent, confidence\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nüìù Test Predictions:\\n\")\n",
    "for query in test_queries:\n",
    "    intent, confidence = predict(query)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"  ‚ûú Intent: {intent}\")\n",
    "    print(f\"  ‚ûú Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"‚úÖ Model testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4073f",
   "metadata": {},
   "source": [
    "# üì• STEP 7: Download Model\n",
    "\n",
    "**Download model yang sudah di-training**\n",
    "\n",
    "File akan di-download sebagai `bert_model.zip` (~500-700 MB)\n",
    "\n",
    "‚è±Ô∏è Waktu download: ~2-5 menit (tergantung koneksi internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== DOWNLOAD MODEL ====================\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"üì¶ Preparing model for download...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Zip the model directory\n",
    "print(\"üîÑ Compressing model files...\")\n",
    "shutil.make_archive('bert_model', 'zip', OUTPUT_DIR)\n",
    "\n",
    "print(\"‚úÖ Model compressed successfully!\")\n",
    "print(f\"üì¶ File: bert_model.zip\")\n",
    "\n",
    "# Get file size\n",
    "import os\n",
    "file_size = os.path.getsize('bert_model.zip') / (1024 * 1024)\n",
    "print(f\"üìä Size: {file_size:.2f} MB\")\n",
    "\n",
    "print(\"\\nüöÄ Starting download...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Download\n",
    "files.download('bert_model.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ MODEL DOWNLOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"1. Extract bert_model.zip\")\n",
    "print(\"2. Copy isi folder ke: '/data/bert_model/'\")\n",
    "print(\"3. Struktur folder harus seperti:\")\n",
    "print(\"   data/bert_model/\")\n",
    "print(\"   ‚îú‚îÄ‚îÄ config.json\")\n",
    "print(\"   ‚îú‚îÄ‚îÄ model.safetensors\")\n",
    "print(\"   ‚îú‚îÄ‚îÄ tokenizer.json\")\n",
    "print(\"   ‚îú‚îÄ‚îÄ label_encoder.pkl\")\n",
    "print(\"   ‚îî‚îÄ‚îÄ ...\")\n",
    "print(\"\\n4. Test model di local:\")\n",
    "print(\"   python -c \\\"from main import get_hybrid_nlu; print('OK')\\\"\")\n",
    "print(\"\\n5. Jalankan server:\")\n",
    "print(\"   uvicorn app:app --reload\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüéâ SELESAI! Model siap digunakan di local!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaff3e0",
   "metadata": {},
   "source": [
    "# üíæ BONUS: Backup ke Google Drive (Optional)\n",
    "\n",
    "Jika ingin backup model ke Google Drive untuk keamanan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f234f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== BACKUP TO GOOGLE DRIVE ====================\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"üìÇ Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create backup directory\n",
    "backup_dir = '/content/drive/MyDrive/fira-bot-backup'\n",
    "!mkdir -p \"{backup_dir}\"\n",
    "\n",
    "# Copy model\n",
    "print(\"\\nüíæ Backing up model to Google Drive...\")\n",
    "!cp -r {OUTPUT_DIR} \"{backup_dir}/\"\n",
    "\n",
    "print(\"\\n‚úÖ Model backed up successfully!\")\n",
    "print(f\"üìÅ Location: {backup_dir}/bert_model\")\n",
    "print(\"\\nüí° Model akan tersimpan permanen di Google Drive Anda\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
